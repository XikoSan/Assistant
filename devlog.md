DevLog
04.05.2025 — День 1
Проект инициализирован, чистый репозиторий.
Модель LLaMA 3 запущена локально через Ollama.
Написана функция ask_llama(prompt) с обработкой ошибок.
Проведён первый тест: логичный и стабильный ответ модели.
Выводы: модель продолжает текст; контекст формируется вручную; API — интерфейс, а не цель.
09.05.2025 — День 2
Реализована функция build_chat_history(), превращающая историю в prompt.
Создан интерактивный main.py: ввод → prompt → ответ → вывод.
Добавлена долговременная память: загрузка и сохранение chat_history в history.json.
Проверено: история сохраняется и подхватывается между запусками.
Запланирован следующий шаг: FastAPI‑эндпоинт /chat и подключение Web‑UI.
День 3 (13 мая 2025)
Что сделано	Зачем
Установлены FastAPI + Uvicorn + Pydantic	подготовка веб-слоя
Создан api.py, объявлен app = FastAPI()	базовое приложение
Роут GET /ping	живая проверка сервера
Роут POST /chat
 • валидация ChatRequest / ChatResponse
 • добавление реплики в chat_history
 • формирование prompt → ask_llama()
 • возврат JSON-ответа	первый внешний интерфейс ассистента
Включён автоперезапуск uvicorn … --reload	удобная разработка
Swagger UI доступен на /docs	ручное тестирование
